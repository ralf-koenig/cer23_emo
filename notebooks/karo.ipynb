{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Text Classification for Emotions using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install datasets huggingface_hub ipywidgets evaluate 'transformers[torch]' torch xformers plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# We need the sys package to load modules from another directory:\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from preprocessing.preprocessors import *\n",
    "\n",
    "import random\n",
    "import evaluate\n",
    "\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plotnine import ggplot, aes, geom_tile, coord_flip,theme,geom_line,labs,element_text\n",
    "from plotnine import scale_x_discrete,geom_vline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>admiration</th>\n",
       "      <th>amusement</th>\n",
       "      <th>anger</th>\n",
       "      <th>annoyance</th>\n",
       "      <th>approval</th>\n",
       "      <th>...</th>\n",
       "      <th>love</th>\n",
       "      <th>nervousness</th>\n",
       "      <th>optimism</th>\n",
       "      <th>pride</th>\n",
       "      <th>realization</th>\n",
       "      <th>relief</th>\n",
       "      <th>remorse</th>\n",
       "      <th>sadness</th>\n",
       "      <th>surprise</th>\n",
       "      <th>neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Right? Considering it’s such an important docu...</td>\n",
       "      <td>eespn2i</td>\n",
       "      <td>ImperialBoss</td>\n",
       "      <td>TrueReddit</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211219</th>\n",
       "      <td>Well, I'm glad you're out of all that now. How...</td>\n",
       "      <td>ed89acy</td>\n",
       "      <td>pompompompi</td>\n",
       "      <td>raisedbynarcissists</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211220</th>\n",
       "      <td>Everyone likes [NAME].</td>\n",
       "      <td>ee6pagw</td>\n",
       "      <td>Senshado</td>\n",
       "      <td>heroesofthestorm</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211221</th>\n",
       "      <td>Well when you’ve imported about a gazillion of...</td>\n",
       "      <td>ef28nod</td>\n",
       "      <td>5inchloser</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211222</th>\n",
       "      <td>That looks amazing</td>\n",
       "      <td>ee8hse1</td>\n",
       "      <td>springt1me</td>\n",
       "      <td>shittyfoodporn</td>\n",
       "      <td>70</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211223</th>\n",
       "      <td>The FDA has plenty to criticize. But like here...</td>\n",
       "      <td>edrhoxh</td>\n",
       "      <td>enamedata</td>\n",
       "      <td>medicine</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171820 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text       id  \\\n",
       "0                                         That game hurt.  eew5j0j   \n",
       "2          You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                      Man I love reddit.  eeibobj   \n",
       "4       [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "5       Right? Considering it’s such an important docu...  eespn2i   \n",
       "...                                                   ...      ...   \n",
       "211219  Well, I'm glad you're out of all that now. How...  ed89acy   \n",
       "211220                             Everyone likes [NAME].  ee6pagw   \n",
       "211221  Well when you’ve imported about a gazillion of...  ef28nod   \n",
       "211222                                 That looks amazing  ee8hse1   \n",
       "211223  The FDA has plenty to criticize. But like here...  edrhoxh   \n",
       "\n",
       "                     author            subreddit  rater_id  admiration  \\\n",
       "0                     Brdd9                  nrl         1           0   \n",
       "2                  Labalool          confessions        37           0   \n",
       "3             MrsRobertshaw             facepalm        18           0   \n",
       "4       American_Fascist713  starwarsspeculation         2           0   \n",
       "5              ImperialBoss           TrueReddit        61           0   \n",
       "...                     ...                  ...       ...         ...   \n",
       "211219          pompompompi  raisedbynarcissists         2           0   \n",
       "211220             Senshado     heroesofthestorm        16           0   \n",
       "211221           5inchloser          nottheonion        15           0   \n",
       "211222           springt1me       shittyfoodporn        70           1   \n",
       "211223            enamedata             medicine         4           0   \n",
       "\n",
       "        amusement  anger  annoyance  approval  ...  love  nervousness  \\\n",
       "0               0      0          0         0  ...     0            0   \n",
       "2               0      0          0         0  ...     0            0   \n",
       "3               0      0          0         0  ...     1            0   \n",
       "4               0      0          0         0  ...     0            0   \n",
       "5               0      0          0         0  ...     0            0   \n",
       "...           ...    ...        ...       ...  ...   ...          ...   \n",
       "211219          0      0          0         0  ...     0            0   \n",
       "211220          0      0          0         0  ...     1            0   \n",
       "211221          0      0          0         0  ...     0            0   \n",
       "211222          0      0          0         0  ...     0            0   \n",
       "211223          0      1          0         0  ...     0            0   \n",
       "\n",
       "        optimism  pride  realization  relief  remorse  sadness  surprise  \\\n",
       "0              0      0            0       0        0        1         0   \n",
       "2              0      0            0       0        0        0         0   \n",
       "3              0      0            0       0        0        0         0   \n",
       "4              0      0            0       0        0        0         0   \n",
       "5              0      0            0       0        0        0         0   \n",
       "...          ...    ...          ...     ...      ...      ...       ...   \n",
       "211219         0      0            0       0        0        0         0   \n",
       "211220         0      0            0       0        0        0         0   \n",
       "211221         0      0            0       0        0        0         0   \n",
       "211222         0      0            0       0        0        0         0   \n",
       "211223         0      0            0       0        0        0         0   \n",
       "\n",
       "        neutral  \n",
       "0             0  \n",
       "2             1  \n",
       "3             0  \n",
       "4             1  \n",
       "5             0  \n",
       "...         ...  \n",
       "211219        0  \n",
       "211220        0  \n",
       "211221        0  \n",
       "211222        0  \n",
       "211223        0  \n",
       "\n",
       "[171820 rows x 33 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/GoEmotions.csv\")\n",
    "df_clean = clean_df(df)\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>level0</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>dis_sad</td>\n",
       "      <td>dis_sad_gri</td>\n",
       "      <td>rem_emb_dis_sad_gri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>37</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>18</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>exc_joy_lov</td>\n",
       "      <td>amu_exc_joy_lov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Right? Considering it’s such an important docu...</td>\n",
       "      <td>eespn2i</td>\n",
       "      <td>ImperialBoss</td>\n",
       "      <td>TrueReddit</td>\n",
       "      <td>61</td>\n",
       "      <td>gratitude</td>\n",
       "      <td>gra_rel</td>\n",
       "      <td>pri_adm_gra_rel</td>\n",
       "      <td>pri_adm_gra_rel_app_rea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211219</th>\n",
       "      <td>Well, I'm glad you're out of all that now. How...</td>\n",
       "      <td>ed89acy</td>\n",
       "      <td>pompompompi</td>\n",
       "      <td>raisedbynarcissists</td>\n",
       "      <td>2</td>\n",
       "      <td>joy</td>\n",
       "      <td>exc_joy</td>\n",
       "      <td>exc_joy_lov</td>\n",
       "      <td>amu_exc_joy_lov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211220</th>\n",
       "      <td>Everyone likes [NAME].</td>\n",
       "      <td>ee6pagw</td>\n",
       "      <td>Senshado</td>\n",
       "      <td>heroesofthestorm</td>\n",
       "      <td>16</td>\n",
       "      <td>love</td>\n",
       "      <td>love</td>\n",
       "      <td>exc_joy_lov</td>\n",
       "      <td>amu_exc_joy_lov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211221</th>\n",
       "      <td>Well when you’ve imported about a gazillion of...</td>\n",
       "      <td>ef28nod</td>\n",
       "      <td>5inchloser</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>15</td>\n",
       "      <td>caring</td>\n",
       "      <td>caring</td>\n",
       "      <td>des_opt_car</td>\n",
       "      <td>des_opt_car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211222</th>\n",
       "      <td>That looks amazing</td>\n",
       "      <td>ee8hse1</td>\n",
       "      <td>springt1me</td>\n",
       "      <td>shittyfoodporn</td>\n",
       "      <td>70</td>\n",
       "      <td>admiration</td>\n",
       "      <td>pri_adm</td>\n",
       "      <td>pri_adm_gra_rel</td>\n",
       "      <td>pri_adm_gra_rel_app_rea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211223</th>\n",
       "      <td>The FDA has plenty to criticize. But like here...</td>\n",
       "      <td>edrhoxh</td>\n",
       "      <td>enamedata</td>\n",
       "      <td>medicine</td>\n",
       "      <td>4</td>\n",
       "      <td>anger</td>\n",
       "      <td>ang_ann</td>\n",
       "      <td>dis_ang_ann</td>\n",
       "      <td>dis_ang_ann_dis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171820 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text       id  \\\n",
       "0                                         That game hurt.  eew5j0j   \n",
       "2          You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                      Man I love reddit.  eeibobj   \n",
       "4       [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "5       Right? Considering it’s such an important docu...  eespn2i   \n",
       "...                                                   ...      ...   \n",
       "211219  Well, I'm glad you're out of all that now. How...  ed89acy   \n",
       "211220                             Everyone likes [NAME].  ee6pagw   \n",
       "211221  Well when you’ve imported about a gazillion of...  ef28nod   \n",
       "211222                                 That looks amazing  ee8hse1   \n",
       "211223  The FDA has plenty to criticize. But like here...  edrhoxh   \n",
       "\n",
       "                     author            subreddit  rater_id      level0  \\\n",
       "0                     Brdd9                  nrl         1     sadness   \n",
       "2                  Labalool          confessions        37     neutral   \n",
       "3             MrsRobertshaw             facepalm        18        love   \n",
       "4       American_Fascist713  starwarsspeculation         2     neutral   \n",
       "5              ImperialBoss           TrueReddit        61   gratitude   \n",
       "...                     ...                  ...       ...         ...   \n",
       "211219          pompompompi  raisedbynarcissists         2         joy   \n",
       "211220             Senshado     heroesofthestorm        16        love   \n",
       "211221           5inchloser          nottheonion        15      caring   \n",
       "211222           springt1me       shittyfoodporn        70  admiration   \n",
       "211223            enamedata             medicine         4       anger   \n",
       "\n",
       "         level1           level2                   level3  \n",
       "0       dis_sad      dis_sad_gri      rem_emb_dis_sad_gri  \n",
       "2       neutral          neutral                  neutral  \n",
       "3          love      exc_joy_lov          amu_exc_joy_lov  \n",
       "4       neutral          neutral                  neutral  \n",
       "5       gra_rel  pri_adm_gra_rel  pri_adm_gra_rel_app_rea  \n",
       "...         ...              ...                      ...  \n",
       "211219  exc_joy      exc_joy_lov          amu_exc_joy_lov  \n",
       "211220     love      exc_joy_lov          amu_exc_joy_lov  \n",
       "211221   caring      des_opt_car              des_opt_car  \n",
       "211222  pri_adm  pri_adm_gra_rel  pri_adm_gra_rel_app_rea  \n",
       "211223  ang_ann      dis_ang_ann          dis_ang_ann_dis  \n",
       "\n",
       "[171820 rows x 9 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_df = create_clustered_df(df_clean)\n",
    "clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>level0</th>\n",
       "      <th>plutchik</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>That game hurt.</td>\n",
       "      <td>eew5j0j</td>\n",
       "      <td>Brdd9</td>\n",
       "      <td>nrl</td>\n",
       "      <td>1</td>\n",
       "      <td>sadness</td>\n",
       "      <td>betrübt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You do right, if you don't care then fuck 'em!</td>\n",
       "      <td>ed2mah1</td>\n",
       "      <td>Labalool</td>\n",
       "      <td>confessions</td>\n",
       "      <td>37</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Man I love reddit.</td>\n",
       "      <td>eeibobj</td>\n",
       "      <td>MrsRobertshaw</td>\n",
       "      <td>facepalm</td>\n",
       "      <td>18</td>\n",
       "      <td>love</td>\n",
       "      <td>verliebt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[NAME] was nowhere near them, he was by the Fa...</td>\n",
       "      <td>eda6yn6</td>\n",
       "      <td>American_Fascist713</td>\n",
       "      <td>starwarsspeculation</td>\n",
       "      <td>2</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Right? Considering it’s such an important docu...</td>\n",
       "      <td>eespn2i</td>\n",
       "      <td>ImperialBoss</td>\n",
       "      <td>TrueReddit</td>\n",
       "      <td>61</td>\n",
       "      <td>gratitude</td>\n",
       "      <td>ehrfürchtig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211219</th>\n",
       "      <td>Well, I'm glad you're out of all that now. How...</td>\n",
       "      <td>ed89acy</td>\n",
       "      <td>pompompompi</td>\n",
       "      <td>raisedbynarcissists</td>\n",
       "      <td>2</td>\n",
       "      <td>joy</td>\n",
       "      <td>begeistert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211220</th>\n",
       "      <td>Everyone likes [NAME].</td>\n",
       "      <td>ee6pagw</td>\n",
       "      <td>Senshado</td>\n",
       "      <td>heroesofthestorm</td>\n",
       "      <td>16</td>\n",
       "      <td>love</td>\n",
       "      <td>verliebt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211221</th>\n",
       "      <td>Well when you’ve imported about a gazillion of...</td>\n",
       "      <td>ef28nod</td>\n",
       "      <td>5inchloser</td>\n",
       "      <td>nottheonion</td>\n",
       "      <td>15</td>\n",
       "      <td>caring</td>\n",
       "      <td>bewundernd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211222</th>\n",
       "      <td>That looks amazing</td>\n",
       "      <td>ee8hse1</td>\n",
       "      <td>springt1me</td>\n",
       "      <td>shittyfoodporn</td>\n",
       "      <td>70</td>\n",
       "      <td>admiration</td>\n",
       "      <td>bewundernd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211223</th>\n",
       "      <td>The FDA has plenty to criticize. But like here...</td>\n",
       "      <td>edrhoxh</td>\n",
       "      <td>enamedata</td>\n",
       "      <td>medicine</td>\n",
       "      <td>4</td>\n",
       "      <td>anger</td>\n",
       "      <td>wütend</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171820 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text       id  \\\n",
       "0                                         That game hurt.  eew5j0j   \n",
       "2          You do right, if you don't care then fuck 'em!  ed2mah1   \n",
       "3                                      Man I love reddit.  eeibobj   \n",
       "4       [NAME] was nowhere near them, he was by the Fa...  eda6yn6   \n",
       "5       Right? Considering it’s such an important docu...  eespn2i   \n",
       "...                                                   ...      ...   \n",
       "211219  Well, I'm glad you're out of all that now. How...  ed89acy   \n",
       "211220                             Everyone likes [NAME].  ee6pagw   \n",
       "211221  Well when you’ve imported about a gazillion of...  ef28nod   \n",
       "211222                                 That looks amazing  ee8hse1   \n",
       "211223  The FDA has plenty to criticize. But like here...  edrhoxh   \n",
       "\n",
       "                     author            subreddit  rater_id      level0  \\\n",
       "0                     Brdd9                  nrl         1     sadness   \n",
       "2                  Labalool          confessions        37     neutral   \n",
       "3             MrsRobertshaw             facepalm        18        love   \n",
       "4       American_Fascist713  starwarsspeculation         2     neutral   \n",
       "5              ImperialBoss           TrueReddit        61   gratitude   \n",
       "...                     ...                  ...       ...         ...   \n",
       "211219          pompompompi  raisedbynarcissists         2         joy   \n",
       "211220             Senshado     heroesofthestorm        16        love   \n",
       "211221           5inchloser          nottheonion        15      caring   \n",
       "211222           springt1me       shittyfoodporn        70  admiration   \n",
       "211223            enamedata             medicine         4       anger   \n",
       "\n",
       "           plutchik  \n",
       "0           betrübt  \n",
       "2           neutral  \n",
       "3          verliebt  \n",
       "4           neutral  \n",
       "5       ehrfürchtig  \n",
       "...             ...  \n",
       "211219   begeistert  \n",
       "211220     verliebt  \n",
       "211221   bewundernd  \n",
       "211222   bewundernd  \n",
       "211223       wütend  \n",
       "\n",
       "[171820 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plutchik_df = create_plutchik_df(df_clean)\n",
    "plutchik_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for level 0 -> 27 emotions\n",
    "following: https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['sadness', 'neutral', 'love', 'gratitude', 'disapproval',\n",
       "       'amusement', 'disappointment', 'realization', 'admiration',\n",
       "       'annoyance', 'confusion', 'optimism', 'excitement', 'caring',\n",
       "       'remorse', 'joy', 'approval', 'embarrassment', 'surprise',\n",
       "       'curiosity', 'anger', 'grief', 'disgust', 'pride', 'desire',\n",
       "       'relief', 'fear', 'nervousness'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clustered_df.level0.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\") # no differentiation between upper and lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'sadness', 1: 'neutral', 2: 'love', 3: 'gratitude', 4: 'disapproval',\n",
    "       5: 'amusement', 6: 'disappointment', 7: 'realization', 8: 'admiration', 9:\n",
    "       'annoyance', 10: 'confusion', 11: 'optimism', 12: 'excitement', 13: 'caring',\n",
    "       14: 'remorse', 15: 'joy', 16: 'approval', 17: 'embarrassment', 18: 'surprise',\n",
    "       19: 'curiosity', 20: 'anger', 21: 'grief', 22: 'disgust', 23: 'pride', 24: 'desire',\n",
    "       25: 'relief', 26: 'fear', 27: 'nervousness'}\n",
    "label2id = {'sadness': 0, 'neutral': 1, 'love': 2, 'gratitude': 3, 'disapproval': 4,\n",
    "       'amusement': 5, 'disappointment': 6, 'realization': 7, 'admiration': 8,\n",
    "       'annoyance': 9, 'confusion': 10, 'optimism': 11, 'excitement': 12, 'caring': 13,\n",
    "       'remorse': 14, 'joy': 15, 'approval': 16, 'embarrassment': 17, 'surprise': 18,\n",
    "       'curiosity': 19, 'anger': 20, 'grief': 21, 'disgust': 22, 'pride': 23, 'desire': 24,\n",
    "       'relief': 25, 'fear': 26, 'nervousness': 27}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>level0</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>label0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26641</th>\n",
       "      <td>if HL in offensive stance dodges it, then does...</td>\n",
       "      <td>eef45jf</td>\n",
       "      <td>GaolGhoul</td>\n",
       "      <td>CompetitiveForHonor</td>\n",
       "      <td>17</td>\n",
       "      <td>realization</td>\n",
       "      <td>app_rea</td>\n",
       "      <td>app_rea</td>\n",
       "      <td>pri_adm_gra_rel_app_rea</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190616</th>\n",
       "      <td>No. It IS her condo and she CAN smoke inside h...</td>\n",
       "      <td>ed71emb</td>\n",
       "      <td>Applesaucedontlie</td>\n",
       "      <td>legaladvice</td>\n",
       "      <td>4</td>\n",
       "      <td>approval</td>\n",
       "      <td>app_rea</td>\n",
       "      <td>app_rea</td>\n",
       "      <td>pri_adm_gra_rel_app_rea</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52488</th>\n",
       "      <td>Love those clips on YouTube, does this add any...</td>\n",
       "      <td>eczl7i9</td>\n",
       "      <td>toxicbrew</td>\n",
       "      <td>netflix</td>\n",
       "      <td>46</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>ang_ann</td>\n",
       "      <td>dis_ang_ann</td>\n",
       "      <td>dis_ang_ann_dis</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190939</th>\n",
       "      <td>Don't know if it's a full moon tonight, but ho...</td>\n",
       "      <td>ed4708f</td>\n",
       "      <td>sritte02</td>\n",
       "      <td>TalesFromRetail</td>\n",
       "      <td>36</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>ang_ann</td>\n",
       "      <td>dis_ang_ann</td>\n",
       "      <td>dis_ang_ann_dis</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27240</th>\n",
       "      <td>Hello!!</td>\n",
       "      <td>ees7cfh</td>\n",
       "      <td>sassypuff01</td>\n",
       "      <td>antiMLM</td>\n",
       "      <td>18</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131015</th>\n",
       "      <td>She was HOPING she was gone!</td>\n",
       "      <td>eeeoavh</td>\n",
       "      <td>RealHausFrau</td>\n",
       "      <td>loveafterlockup</td>\n",
       "      <td>30</td>\n",
       "      <td>desire</td>\n",
       "      <td>des_opt</td>\n",
       "      <td>des_opt_car</td>\n",
       "      <td>des_opt_car</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201100</th>\n",
       "      <td>I don't really understand what they are even p...</td>\n",
       "      <td>eez4aas</td>\n",
       "      <td>Nuclearfrog</td>\n",
       "      <td>ukpolitics</td>\n",
       "      <td>35</td>\n",
       "      <td>confusion</td>\n",
       "      <td>cur_con</td>\n",
       "      <td>sur_cur_con</td>\n",
       "      <td>sur_cur_con</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145855</th>\n",
       "      <td>Yeah, surely this is *exactly* what democracy ...</td>\n",
       "      <td>ee5gal8</td>\n",
       "      <td>NearbyBush</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>81</td>\n",
       "      <td>curiosity</td>\n",
       "      <td>cur_con</td>\n",
       "      <td>sur_cur_con</td>\n",
       "      <td>sur_cur_con</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182196</th>\n",
       "      <td>Eastern suburbs, the mullet never went out of ...</td>\n",
       "      <td>edk0ssq</td>\n",
       "      <td>_Dindu__Nuffin_</td>\n",
       "      <td>perth</td>\n",
       "      <td>3</td>\n",
       "      <td>approval</td>\n",
       "      <td>app_rea</td>\n",
       "      <td>app_rea</td>\n",
       "      <td>pri_adm_gra_rel_app_rea</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188539</th>\n",
       "      <td>Sorry, mom took it out of the oven already and...</td>\n",
       "      <td>ef64v0v</td>\n",
       "      <td>Lesurous</td>\n",
       "      <td>terriblefacebookmemes</td>\n",
       "      <td>51</td>\n",
       "      <td>remorse</td>\n",
       "      <td>rem_emb</td>\n",
       "      <td>rem_emb</td>\n",
       "      <td>rem_emb_dis_sad_gri</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text       id  \\\n",
       "26641   if HL in offensive stance dodges it, then does...  eef45jf   \n",
       "190616  No. It IS her condo and she CAN smoke inside h...  ed71emb   \n",
       "52488   Love those clips on YouTube, does this add any...  eczl7i9   \n",
       "190939  Don't know if it's a full moon tonight, but ho...  ed4708f   \n",
       "27240                                             Hello!!  ees7cfh   \n",
       "...                                                   ...      ...   \n",
       "131015                       She was HOPING she was gone!  eeeoavh   \n",
       "201100  I don't really understand what they are even p...  eez4aas   \n",
       "145855  Yeah, surely this is *exactly* what democracy ...  ee5gal8   \n",
       "182196  Eastern suburbs, the mullet never went out of ...  edk0ssq   \n",
       "188539  Sorry, mom took it out of the oven already and...  ef64v0v   \n",
       "\n",
       "                   author              subreddit  rater_id       level0  \\\n",
       "26641           GaolGhoul    CompetitiveForHonor        17  realization   \n",
       "190616  Applesaucedontlie            legaladvice         4     approval   \n",
       "52488           toxicbrew                netflix        46    annoyance   \n",
       "190939           sritte02        TalesFromRetail        36    annoyance   \n",
       "27240         sassypuff01                antiMLM        18      neutral   \n",
       "...                   ...                    ...       ...          ...   \n",
       "131015       RealHausFrau        loveafterlockup        30       desire   \n",
       "201100        Nuclearfrog             ukpolitics        35    confusion   \n",
       "145855         NearbyBush               Scotland        81    curiosity   \n",
       "182196    _Dindu__Nuffin_                  perth         3     approval   \n",
       "188539           Lesurous  terriblefacebookmemes        51      remorse   \n",
       "\n",
       "         level1       level2                   level3  label0  \n",
       "26641   app_rea      app_rea  pri_adm_gra_rel_app_rea       7  \n",
       "190616  app_rea      app_rea  pri_adm_gra_rel_app_rea      16  \n",
       "52488   ang_ann  dis_ang_ann          dis_ang_ann_dis       9  \n",
       "190939  ang_ann  dis_ang_ann          dis_ang_ann_dis       9  \n",
       "27240   neutral      neutral                  neutral       1  \n",
       "...         ...          ...                      ...     ...  \n",
       "131015  des_opt  des_opt_car              des_opt_car      24  \n",
       "201100  cur_con  sur_cur_con              sur_cur_con      10  \n",
       "145855  cur_con  sur_cur_con              sur_cur_con      19  \n",
       "182196  app_rea      app_rea  pri_adm_gra_rel_app_rea      16  \n",
       "188539  rem_emb      rem_emb      rem_emb_dis_sad_gri      14  \n",
       "\n",
       "[1000 rows x 10 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a sample for lokal tests\n",
    "random.seed(123)\n",
    "dataset = clustered_df.sample(n=1000, replace=False)\n",
    "dataset[\"label0\"] = dataset[\"level0\"].map(label2id.get) # to add column label map individual entries of emotions to ID\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d453c58baca5492096169fe33efac0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/798 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a97a2eed17419bac401dddd15299aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/202 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# split the training data\n",
    "\n",
    "training_data = dataset.groupby(\"level0\").sample(frac=0.8, random_state=25) # stratified sampling\n",
    "testing_data = dataset.drop(training_data.index)\n",
    "\n",
    "training_data = Dataset.from_pandas(training_data) # create transformers compatible dataset from dataframe\n",
    "testing_data = Dataset.from_pandas(testing_data)\n",
    "\n",
    "def tokenize_function(examples): # replace representation of data, convert column text to tensor-based representation\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_training_data = training_data.map(tokenize_function, batched=True) # convert text to tensor form\n",
    "tokenized_testing_data = testing_data.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'id', 'author', 'subreddit', 'rater_id', 'level0', 'level1', 'level2', 'level3', 'label0', '__index_level_0__', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 798\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>level0</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>label0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>110753</th>\n",
       "      <td>No idea what this means but the fact that you said sweetie lets me know that you’re probably really obnoxious or just being sarcastic</td>\n",
       "      <td>edxzz1l</td>\n",
       "      <td>Plasma454345</td>\n",
       "      <td>iamverysmart</td>\n",
       "      <td>72</td>\n",
       "      <td>annoyance</td>\n",
       "      <td>ang_ann</td>\n",
       "      <td>dis_ang_ann</td>\n",
       "      <td>dis_ang_ann_dis</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                         text  \\\n",
       "110753  No idea what this means but the fact that you said sweetie lets me know that you’re probably really obnoxious or just being sarcastic   \n",
       "\n",
       "             id        author     subreddit  rater_id     level0   level1  \\\n",
       "110753  edxzz1l  Plasma454345  iamverysmart        72  annoyance  ang_ann   \n",
       "\n",
       "             level2           level3  label0  \n",
       "110753  dis_ang_ann  dis_ang_ann_dis       9  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth = None # default value is 50, max would be \"None\"\n",
    "pd.set_option('display.max_rows', 50) # default value is 10, max would be \"None\"\n",
    "\n",
    "annoyance_example = random.sample(list(dataset.id[dataset.level0 == \"annoyance\"]), k=1) # example for annoyance\n",
    "dataset.query('id==@annoyance_example')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>rater_id</th>\n",
       "      <th>level0</th>\n",
       "      <th>level1</th>\n",
       "      <th>level2</th>\n",
       "      <th>level3</th>\n",
       "      <th>label0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173766</th>\n",
       "      <td>Bro, too close to home. Thing is I wish I could just chill where this monk is at, with a lot of open space.</td>\n",
       "      <td>edx2c5f</td>\n",
       "      <td>BlueLanternSupes</td>\n",
       "      <td>2meirl4meirl</td>\n",
       "      <td>63</td>\n",
       "      <td>desire</td>\n",
       "      <td>des_opt</td>\n",
       "      <td>des_opt_car</td>\n",
       "      <td>des_opt_car</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                               text  \\\n",
       "173766  Bro, too close to home. Thing is I wish I could just chill where this monk is at, with a lot of open space.   \n",
       "\n",
       "             id            author     subreddit  rater_id  level0   level1  \\\n",
       "173766  edx2c5f  BlueLanternSupes  2meirl4meirl        63  desire  des_opt   \n",
       "\n",
       "             level2       level3  label0  \n",
       "173766  des_opt_car  des_opt_car      24  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desire_example = random.sample(list(dataset.id[dataset.level0 == \"desire\"]), k=1) # example for desire\n",
    "dataset.query('id==@desire_example') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral           305\n",
      "approval           59\n",
      "annoyance          56\n",
      "admiration         54\n",
      "disapproval        48\n",
      "gratitude          41\n",
      "confusion          38\n",
      "curiosity          37\n",
      "love               34\n",
      "surprise           30\n",
      "anger              30\n",
      "disappointment     28\n",
      "optimism           27\n",
      "sadness            26\n",
      "amusement          24\n",
      "excitement         23\n",
      "desire             23\n",
      "joy                23\n",
      "caring             21\n",
      "disgust            19\n",
      "realization        18\n",
      "embarrassment      11\n",
      "relief              8\n",
      "remorse             5\n",
      "fear                5\n",
      "nervousness         4\n",
      "grief               2\n",
      "pride               1\n",
      "Name: level0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check if data set is balanced\n",
    "classCounts = dataset.level0.value_counts() \n",
    "print(classCounts)\n",
    "# -> not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numberOfDocuments = len(dataset)\n",
    "numberOfDocuments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Classifier\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorWithPadding(tokenizer=DistilBertTokenizerFast(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True), padding=True, max_length=None, pad_to_multiple_of=None, return_tensors='pt')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # Padding -> map all tensors to the same size\n",
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b5dee291e8e437ba8621d42a5b62e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy = evaluate.load(\"accuracy\") # define evaluation method -> quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred): # function calculation metric\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'classifier.weight', 'pre_classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=28, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a9f28bb6e87435f81a7815a94035b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 31\u001b[0m\n\u001b[0;32m     19\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     20\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[0;32m     21\u001b[0m     args\u001b[39m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics  \n\u001b[0;32m     27\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[39m#checkpointing\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m#use cuda\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m trainer\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1663\u001b[0m     args\u001b[39m=\u001b[39margs,\n\u001b[0;32m   1664\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   1665\u001b[0m     trial\u001b[39m=\u001b[39mtrial,\n\u001b[0;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   1667\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1931\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\trainer.py:2699\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2698\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2699\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   2701\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2702\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\trainer.py:2731\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2729\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2730\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2731\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2732\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2733\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2734\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:763\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    756\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    757\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m    758\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m    760\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    761\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m--> 763\u001b[0m distilbert_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistilbert(\n\u001b[0;32m    764\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m    765\u001b[0m     attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    766\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m    767\u001b[0m     inputs_embeds\u001b[39m=\u001b[39minputs_embeds,\n\u001b[0;32m    768\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    769\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    770\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    771\u001b[0m )\n\u001b[0;32m    772\u001b[0m hidden_state \u001b[39m=\u001b[39m distilbert_output[\u001b[39m0\u001b[39m]  \u001b[39m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[0;32m    773\u001b[0m pooled_output \u001b[39m=\u001b[39m hidden_state[:, \u001b[39m0\u001b[39m]  \u001b[39m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:583\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    579\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    581\u001b[0m embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[39m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer(\n\u001b[0;32m    584\u001b[0m     x\u001b[39m=\u001b[39membeddings,\n\u001b[0;32m    585\u001b[0m     attn_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m    586\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m    587\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    588\u001b[0m     output_hidden_states\u001b[39m=\u001b[39moutput_hidden_states,\n\u001b[0;32m    589\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[0;32m    590\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:359\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m    357\u001b[0m     all_hidden_states \u001b[39m=\u001b[39m all_hidden_states \u001b[39m+\u001b[39m (hidden_state,)\n\u001b[1;32m--> 359\u001b[0m layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    360\u001b[0m     x\u001b[39m=\u001b[39mhidden_state, attn_mask\u001b[39m=\u001b[39mattn_mask, head_mask\u001b[39m=\u001b[39mhead_mask[i], output_attentions\u001b[39m=\u001b[39moutput_attentions\n\u001b[0;32m    361\u001b[0m )\n\u001b[0;32m    362\u001b[0m hidden_state \u001b[39m=\u001b[39m layer_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    364\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:295\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    286\u001b[0m \u001b[39mParameters:\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[39m    x: torch.tensor(bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    292\u001b[0m \u001b[39m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    294\u001b[0m \u001b[39m# Self-Attention\u001b[39;00m\n\u001b[1;32m--> 295\u001b[0m sa_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention(\n\u001b[0;32m    296\u001b[0m     query\u001b[39m=\u001b[39mx,\n\u001b[0;32m    297\u001b[0m     key\u001b[39m=\u001b[39mx,\n\u001b[0;32m    298\u001b[0m     value\u001b[39m=\u001b[39mx,\n\u001b[0;32m    299\u001b[0m     mask\u001b[39m=\u001b[39mattn_mask,\n\u001b[0;32m    300\u001b[0m     head_mask\u001b[39m=\u001b[39mhead_mask,\n\u001b[0;32m    301\u001b[0m     output_attentions\u001b[39m=\u001b[39moutput_attentions,\n\u001b[0;32m    302\u001b[0m )\n\u001b[0;32m    303\u001b[0m \u001b[39mif\u001b[39;00m output_attentions:\n\u001b[0;32m    304\u001b[0m     sa_output, sa_weights \u001b[39m=\u001b[39m sa_output  \u001b[39m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\HP\\miniconda3\\envs\\emo\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:222\u001b[0m, in \u001b[0;36mMultiHeadSelfAttention.forward\u001b[1;34m(self, query, key, value, mask, head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    220\u001b[0m scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(q, k\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m))  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    221\u001b[0m mask \u001b[39m=\u001b[39m (mask \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mview(mask_reshp)\u001b[39m.\u001b[39mexpand_as(scores)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m scores \u001b[39m=\u001b[39m scores\u001b[39m.\u001b[39mmasked_fill(\n\u001b[0;32m    223\u001b[0m     mask, torch\u001b[39m.\u001b[39mtensor(torch\u001b[39m.\u001b[39mfinfo(scores\u001b[39m.\u001b[39mdtype)\u001b[39m.\u001b[39mmin)\n\u001b[0;32m    224\u001b[0m )  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    226\u001b[0m weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(scores, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n\u001b[0;32m    227\u001b[0m weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(weights)  \u001b[39m# (bs, n_heads, q_length, k_length)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\b\\abs_abjetg6_iu\\croot\\pytorch_1686932924616\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 201326592 bytes."
     ]
    }
   ],
   "source": [
    "# training\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"../models/model_level0\",\n",
    "    learning_rate=2e-5,  # standard\n",
    "    per_device_train_batch_size=16, # size in which chunks are entered into the network, on how many data parallel weights are trained\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\", # save model per epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=False #,\n",
    "    #label_names=[\"level0\"],\n",
    ")\n",
    "\n",
    "# IMPORTANT: Set: Model, dataset, ... , define learning process, metrics, ...\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_training_data,\n",
    "    eval_dataset=tokenized_testing_data,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics  \n",
    ")\n",
    "\n",
    "#checkpointing\n",
    "#use cuda\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"../models/model_level0_1epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"text-classification\", model=\"../models/model_level0_1epoch\",device=0) # method pipeline -> sting for textclassificaton, folder, device (graphics card)\n",
    "results = [classifier(text,truncation=True) for text in tqdm(dataset.text.to_list())] # listcomprehension over all texts, tokenization in model, truncation -> padding too long texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [tmp[0] for tmp in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results # list of dictionaries with labels, score -> decision and how high activation function for decision was\n",
    "pd.DataFrame(results).to_pickle(\"../results/results_level0_1epoch.pkl\")  # convert as dataframe, pick, safe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies = pd.concat([dataset,pd.DataFrame.from_dict(results)],axis=1)\n",
    "# merge classified data with original training data\n",
    "# combine data with training data, concatenate results results and training data\n",
    "# compare -> calculate f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies.to_pickle(\"../results/data_classified_level0_1epoch.pkl\")  \n",
    "# data_classifies = pd.read_pickle(\"../results/data_classified_level0_1epoch.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies # contain goldstandard and ergbnis cluster -> calculate F1, Precision, Recall\n",
    "# label -> assigned by classifier (?????)\n",
    "# level0 -> original label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = data_classifies.query(f'id in {tokenized_testing_data[\"id\"]}')\n",
    "# tokenized_testing_data: daten der Testdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = ['sadness', 'neutral', 'love', 'gratitude', 'disapproval',\n",
    "       'amusement', 'disappointment', 'realization', 'admiration',\n",
    "       'annoyance', 'confusion', 'optimism', 'excitement', 'caring',\n",
    "       'remorse', 'joy', 'approval', 'embarrassment', 'surprise',\n",
    "       'curiosity', 'anger', 'grief', 'disgust', 'pride', 'desire',\n",
    "       'relief', 'fear', 'nervousness']\n",
    "print(classification_report(test_data.level0, test_data.label, target_names=target_names))\n",
    "# level0 -> gold standard , label -> prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Classification/Viz\n",
    "final = pd.concat([dataset, pd.DataFrame.from_dict(results)],axis=1) # attach classified label to data\n",
    "final['label'].value_counts()/final['label'].value_counts().sum() # ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1,12))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
