{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Class Text Classification for Emotions using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install numpy pandas scikit-learn ipykernel jupyter matplotlib seaborn evaluate 'transformers[torch]' transformers pytorch tqdm datasets huggingface_hub ipywidgets torch xformers plotnine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# We need the sys package to load modules from another directory:\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from preprocessing.preprocessors import *\n",
    "from training.bert_func import *\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/GoEmotions.csv\")\n",
    "df_clean = clean_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = df_clean.shape\n",
    "print(f\"The data has {r} row and {c} columns\")\n",
    "df_clean.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_df = create_pivoted_df(df_clean)\n",
    "hierarchical_df = add_hierarchical_levels(pivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = hierarchical_df.shape\n",
    "print(f\"The data has {r} row and {c} columns\")\n",
    "hierarchical_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use majority vote for texts with more than one label (from different raters) to only have one gold standard\n",
    "majority_vote_df = majority_voted_df(hierarchical_df)\n",
    "r, c = majority_vote_df.shape\n",
    "print(f\"The majority voted data has {r} row and {c} columns\")\n",
    "\n",
    "clustered_df = hierarchical_df.merge(majority_vote_df, on=['id', 'level0'], how='inner')\n",
    "\n",
    "majority_vote_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = clustered_df.shape\n",
    "print(f\"The data has {r} row and {c} columns\")\n",
    "clustered_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the rows with distinct values in the 'id' column\n",
    "distinct_df = clustered_df.drop_duplicates(subset='id', keep='first')\n",
    "r, c = distinct_df.shape\n",
    "print(f\"The data has {r} row and {c} columns\")\n",
    "distinct_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a sample for tests\n",
    "# distinct_df = distinct_df.sample(n=1000, replace=False, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "#Classifier\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = \"bert-base-cased\"\n",
    "models_dir = \"../models/bert_base_cased/\"\n",
    "results_dir = \"../results/bert_base_cased/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for level 0 -> 27 +1 emotions\n",
    "following: https://huggingface.co/docs/transformers/tasks/sequence_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None # default value is 50, max would be \"None\"\n",
    "pd.set_option('display.max_rows', 50) # default value is 10, max would be \"None\"\n",
    "\n",
    "sadness_example = random.sample(list(distinct_df.id[distinct_df.level0 == \"sadness\"]), k=1) # example for annoyance\n",
    "distinct_df.query('id==@sadness_example') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data set is balanced\n",
    "classCounts_0 = distinct_df.level0.value_counts()\n",
    "print(classCounts_0)\n",
    "# -> not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfDocuments_0 = len(distinct_df)\n",
    "numberOfDocuments_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_0, results_0, tokenized_testing_data_0, testing_data_0, label2id_0 = get_bert(distinct_df, \"level0\", bert, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data set is balanced\n",
    "classCounts_0 = pd.DataFrame(testing_data_0).level0.value_counts() \n",
    "print(classCounts_0)\n",
    "# -> not balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_0 = pd.DataFrame.from_dict(results_0)\n",
    "df_id_0 =  pd.DataFrame(dataset_0[\"id\"])\n",
    "df_id_0 = df_id_0.reset_index()\n",
    "df_results_0[\"id\"] = df_id_0[\"id\"]\n",
    "df_results_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies_0 = pd.merge(dataset_0, df_results_0, on='id', how='left') # merge classified data with original training data\n",
    "data_classifies_0.rename(columns={'label_y': 'LABEL_pred'}, inplace=True) # contain sgoldstandard and cluster of results\n",
    "data_classifies_0[\"LABEL_pred_num\"] = data_classifies_0[\"LABEL_pred\"].map(label2id_0.get)\n",
    "data_classifies_0.to_pickle(results_dir + \"data_classified_level0.pkl\") \n",
    "data_classifies_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_0 = data_classifies_0.query(f'id in {tokenized_testing_data_0[\"id\"]}')\n",
    "test_data_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = pd.DataFrame(testing_data_0).shape\n",
    "print(f\"The test data has {r} row and {c} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_0 = pd.DataFrame(classification_report(test_data_0.level0, test_data_0.LABEL_pred, output_dict=True)).transpose()\n",
    "report_0.to_csv(results_dir + \"model_level0_report.csv\")\n",
    "print(report_0)\n",
    "# level0 -> gold standard , LABEL_pred -> prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Classification/Viz\n",
    "\n",
    "final_0 = pd.DataFrame(test_data_0['LABEL_pred'].value_counts()/test_data_0['LABEL_pred'].value_counts().sum()) # ratio\n",
    "final_0.to_csv(results_dir + \"model_level0_testdata_frequency.csv\")\n",
    "print(final_0.shape)\n",
    "final_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for level 1 -> 17 + 1 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None # default value is 50, max would be \"None\"\n",
    "pd.set_option('display.max_rows', 50) # default value is 10, max would be \"None\"\n",
    "\n",
    "dis_sad_example = random.sample(list(distinct_df.id[distinct_df.level1 == \"dis_sad\"]), k=1) # example for annoyance\n",
    "distinct_df.query('id==@dis_sad_example')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data set is balanced\n",
    "classCounts_1 = distinct_df.level1.value_counts() \n",
    "print(classCounts_1)\n",
    "# -> not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfDocuments_1 = len(distinct_df)\n",
    "numberOfDocuments_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1, results_1, tokenized_testing_data_1, testing_data_1, label2id_1 = get_bert(distinct_df, \"level1\", bert, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_1 = pd.DataFrame.from_dict(results_1)\n",
    "df_id_1 =  pd.DataFrame(dataset_1[\"id\"])\n",
    "df_id_1 = df_id_1.reset_index()\n",
    "df_results_1[\"id\"] = df_id_1[\"id\"]\n",
    "df_results_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies_1 = pd.merge(dataset_1, df_results_1, on='id', how='left') # merge classified data with original training data\n",
    "data_classifies_1.rename(columns={'label_y': 'LABEL_pred'}, inplace=True) # contain sgoldstandard and cluster of results\n",
    "data_classifies_1[\"LABEL_pred_num\"] = data_classifies_1[\"LABEL_pred\"].map(label2id_1.get)\n",
    "data_classifies_1.to_pickle(results_dir + \"data_classified_level1.pkl\") \n",
    "data_classifies_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_1 = data_classifies_1.query(f'id in {tokenized_testing_data_1[\"id\"]}')\n",
    "test_data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = pd.DataFrame(testing_data_1).shape\n",
    "print(f\"The test data has {r} row and {c} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_1= pd.DataFrame(classification_report(test_data_1.level1, test_data_1.LABEL_pred, output_dict=True)).transpose()\n",
    "report_1.to_csv(results_dir + \"model_level1_report.csv\")\n",
    "print(report_1)\n",
    "# level1 -> gold standard , LABEL_pred -> prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Classification/Viz\n",
    "\n",
    "final_1 = pd.DataFrame(test_data_1['LABEL_pred'].value_counts()/test_data_1['LABEL_pred'].value_counts().sum()) # ratio\n",
    "final_1.to_csv(results_dir + \"model_level1_testdata_frequency.csv\")\n",
    "print(final_1.shape)\n",
    "final_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for level 2 -> 11 + 1 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None # default value is 50, max would be \"None\"\n",
    "pd.set_option('display.max_rows', 50) # default value is 10, max would be \"None\"\n",
    "\n",
    "dis_sad_gri_example = random.sample(list(distinct_df.id[distinct_df.level2 == \"dis_sad_gri\"]), k=1) # example for annoyance\n",
    "distinct_df.query('id==@dis_sad_gri_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data set is balanced\n",
    "classCounts_2 = distinct_df.level2.value_counts() \n",
    "print(classCounts_2)\n",
    "# -> not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfDocuments_2 = len(distinct_df)\n",
    "numberOfDocuments_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2, results_2, tokenized_testing_data_2, testing_data_2, label2id_2 = get_bert(distinct_df, \"level2\", bert, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_2 = pd.DataFrame.from_dict(results_2)\n",
    "df_id_2 =  pd.DataFrame(dataset_2[\"id\"])\n",
    "df_id_2 = df_id_2.reset_index()\n",
    "df_results_2[\"id\"] = df_id_2[\"id\"]\n",
    "df_results_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies_2 = pd.merge(dataset_2, df_results_2, on='id', how='left') # merge classified data with original training data\n",
    "data_classifies_2.rename(columns={'label_y': 'LABEL_pred'}, inplace=True) # contain sgoldstandard and cluster of results\n",
    "data_classifies_2[\"LABEL_pred_num\"] = data_classifies_2[\"LABEL_pred\"].map(label2id_2.get)\n",
    "data_classifies_2.to_pickle(results_dir + \"data_classified_level2.pkl\") \n",
    "data_classifies_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_2 = data_classifies_2.query(f'id in {tokenized_testing_data_2[\"id\"]}')\n",
    "test_data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = pd.DataFrame(testing_data_2).shape\n",
    "print(f\"The test data has {r} row and {c} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_2 = pd.DataFrame(classification_report(test_data_2.level2, test_data_2.LABEL_pred, output_dict=True)).transpose()\n",
    "report_2.to_csv(results_dir + \"model_level2_report.csv\")\n",
    "print(report_2)\n",
    "# level2 -> gold standard , LABEL_pred -> prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Classification/Viz\n",
    "\n",
    "final_2 = pd.DataFrame(test_data_2['LABEL_pred'].value_counts()/test_data_2['LABEL_pred'].value_counts().sum()) # ratio\n",
    "final_2.to_csv(results_dir + \"model_level2_testdata_frequency.csv\")\n",
    "print(final_2.shape)\n",
    "final_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for level 3 -> 7 + 1 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None # default value is 50, max would be \"None\"\n",
    "pd.set_option('display.max_rows', 50) # default value is 10, max would be \"None\"\n",
    "\n",
    "rem_emb_dis_sad_gri_example = random.sample(list(distinct_df.id[distinct_df.level3 == \"rem_emb_dis_sad_gri\"]), k=1) # example for annoyance\n",
    "distinct_df.query('id==@rem_emb_dis_sad_gri_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data set is balanced\n",
    "classCounts_3 = distinct_df.level3.value_counts() \n",
    "print(classCounts_3)\n",
    "# -> not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfDocuments_3 = len(distinct_df)\n",
    "numberOfDocuments_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3, results_3, tokenized_testing_data_3, testing_data_3, label2id_3 = get_bert(distinct_df, \"level3\", bert, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_3 = pd.DataFrame.from_dict(results_3)\n",
    "df_id_3 =  pd.DataFrame(dataset_3[\"id\"])\n",
    "df_id_3 = df_id_3.reset_index()\n",
    "df_results_3[\"id\"] = df_id_3[\"id\"]\n",
    "df_results_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies_3 = pd.merge(dataset_3, df_results_3, on='id', how='left') # merge classified data with original training data\n",
    "data_classifies_3.rename(columns={'label_y': 'LABEL_pred'}, inplace=True) # contain sgoldstandard and cluster of results\n",
    "data_classifies_3[\"LABEL_pred_num\"] = data_classifies_3[\"LABEL_pred\"].map(label2id_3.get)\n",
    "data_classifies_3.to_pickle(results_dir + \"data_classified_level3.pkl\") \n",
    "data_classifies_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_3 = data_classifies_3.query(f'id in {tokenized_testing_data_3[\"id\"]}')\n",
    "test_data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = pd.DataFrame(testing_data_3).shape\n",
    "print(f\"The test data has {r} row and {c} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_3 = pd.DataFrame(classification_report(test_data_3.level3, test_data_3.LABEL_pred, output_dict=True)).transpose()\n",
    "report_3.to_csv(results_dir + \"model_level3_report.csv\")\n",
    "print(report_3)\n",
    "# level3 -> gold standard , LABEL_pred -> prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Classification/Viz\n",
    "\n",
    "final_3 = pd.DataFrame(test_data_3['LABEL_pred'].value_counts()/test_data_3['LABEL_pred'].value_counts().sum()) # ratio\n",
    "final_3.to_csv(results_dir + \"model_level3_testdata_frequency.csv\")\n",
    "print(final_3.shape)\n",
    "final_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT for plutchik -> 14 + 1 emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = None # default value is 50, max would be \"None\"\n",
    "pd.set_option('display.max_rows', 50) # default value is 10, max would be \"None\"\n",
    "\n",
    "grief_example = random.sample(list(distinct_df.id[distinct_df.plutchik == \"grief\"]), k=1) # example for annoyance\n",
    "distinct_df.query('id==@grief_example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if data set is balanced\n",
    "classCounts_p = distinct_df.plutchik.value_counts() \n",
    "print(classCounts_p)\n",
    "# -> not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfDocuments_p = len(distinct_df)\n",
    "numberOfDocuments_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_p, results_p, tokenized_testing_data_p, testing_data_p, label2id_p = get_bert(distinct_df, \"plutchik\", bert, models_dir, results_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_p = pd.DataFrame.from_dict(results_p)\n",
    "df_id_p =  pd.DataFrame(dataset_2[\"id\"])\n",
    "df_id_p = df_id_p.reset_index()\n",
    "df_results_p[\"id\"] = df_id_p[\"id\"]\n",
    "df_results_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_classifies_p = pd.merge(dataset_p, df_results_p, on='id', how='left') # merge classified data with original training data\n",
    "data_classifies_p.rename(columns={'label_y': 'LABEL_pred'}, inplace=True) # contain sgoldstandard and cluster of results\n",
    "data_classifies_p[\"LABEL_pred_num\"] = data_classifies_p[\"LABEL_pred\"].map(label2id_p.get)\n",
    "data_classifies_p.to_pickle(results_dir + \"data_classified_plutchik.pkl\") \n",
    "data_classifies_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_p = data_classifies_p.query(f'id in {tokenized_testing_data_p[\"id\"]}')\n",
    "test_data_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = pd.DataFrame(testing_data_p).shape\n",
    "print(f\"The test data has {r} row and {c} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_p = pd.DataFrame(classification_report(test_data_p.plutchik, test_data_p.LABEL_pred, output_dict=True)).transpose()\n",
    "report_p.to_csv(results_dir + \"model_plutchik_report.csv\")\n",
    "print(report_p)\n",
    "# plutchik -> gold standard , LABEL_pred -> prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final Classification/Viz\n",
    "\n",
    "final_p = pd.DataFrame(test_data_p['LABEL_pred'].value_counts()/test_data_p['LABEL_pred'].value_counts().sum()) # ratio\n",
    "final_p.to_csv(results_dir + \"model_plutchik_testdata_frequency.csv\")\n",
    "print(final_p.shape)\n",
    "final_p"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
